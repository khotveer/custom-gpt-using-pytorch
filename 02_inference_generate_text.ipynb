{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af68762d-d942-4295-ab1f-637122f17529",
   "metadata": {},
   "source": [
    "### set seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1accbc-940e-43b3-b3ee-500c808c916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9b7ec42-48f7-4038-9eb9-97dd50099860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import *\n",
    "from src.tokenizer import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880ea27-e705-4cfa-b9ad-089fc908b11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cabb54e-3175-4e49-948a-6fff9c21eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, config, start_text, encoder, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "    seed = random.randint(0, 10_000)\n",
    "    set_seed(seed)\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode the input text\n",
    "    encoded = encoder.encoder(start_text)\n",
    "    x = torch.tensor(encoded, dtype=torch.long)[None, :].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x if x.size(1) <= config['block_size'] else x[:, -config['block_size']:]\n",
    "\n",
    "        logits, _ = model(x_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, top_indices = torch.topk(logits, top_k)\n",
    "            logits = torch.full_like(logits, float('-inf'))\n",
    "            logits.scatter_(1, top_indices, top_logits)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "    # Decode tokens to text\n",
    "    out = x[0].tolist()\n",
    "    return encoder.decoder(out)\n",
    "\n",
    "def clean_repetition(text):\n",
    "    # Clean up repeated \"Nobel Prize in <field>\"\n",
    "    text = re.sub(r'(the nobel prize in \\w+)( \\1)+', r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Clean other repetitive structures, e.g., \"the the\", \"was was\"\n",
    "    text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n",
    "\n",
    "    # Remove excessive repetition of \"in <year>\" in close succession\n",
    "    text = re.sub(r'(\\d{4},\\s*)\\1+', r'\\1', text)\n",
    "\n",
    "    # Clean up the format if there are multiple \"Nobel Prize\" mentions in different sections\n",
    "    text = re.sub(r'(the nobel prize in \\w+ ){2,}', r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e52703-2282-4c52-a9c5-0f0c082fd828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958ae4b9-6f5f-4d16-82af-1ea5ccc680b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    " 'n_layer': 8,\n",
    " 'n_head': 16,\n",
    " 'n_embd': 512,\n",
    " 'vocab_size': 50257,\n",
    " 'block_size': 128,\n",
    " 'embd_pdrop': 0.1,\n",
    " 'resid_pdrop': 0.1,\n",
    " 'attn_pdrop': 0.1,\n",
    " 'device': 'cpu',\n",
    " 'num_workers': 3,\n",
    " 'max_iters': None,\n",
    " 'batch_size': 64,\n",
    " 'learning_rate': 0.0003,\n",
    " 'betas': (0.9, 0.95),\n",
    " 'weight_decay': 0.1,\n",
    " 'grad_norm_clip': 1.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657d8d40-dfa7-4a8a-a63a-83373755b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 51.02M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b91d5521-7d46-485e-bc5a-0efc3ca1a043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(torch.load('./saved_models/model_shakespeare_new_v5_latest.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a11a6be3-9f1f-46f8-9edd-6aa7ebfb5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b1f28e-c094-4543-9767-91de8c3bc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('./saved_models/encoder_shakespeare_v5.pkl', 'rb') as f:\n",
    "    bpe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc88b6d-140c-4f89-a1a0-883b2b323b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ec58b8-c4a8-44ee-a74c-e981ee5a6a27",
   "metadata": {},
   "source": [
    "### Basic Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6f429a6-73ca-4af6-9d2e-104f9617cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  first citizen\n",
      "output:  first citizen:\n",
      "he cannot help the joy of others, proud disdain,\n",
      "unless the loving welshmen can clear\n",
      "ne'gainst the strong suspicion.\n",
      "\n",
      "second murderer:\n",
      "no, first let's reason with him.\n",
      "\n",
      "clarence:\n",
      "where art thou, keeper? give me a cup of wine.\n",
      "\n",
      "second murderer:\n",
      "you shall have wine enough, my lord, anon.\n",
      "\n",
      "clarence:\n",
      "in god's name, what art thou?\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "input:  hermione:\n",
      "output:  hermione:\n",
      "nay, but you will?\n",
      "\n",
      "polixenes:\n",
      "i may not, verily.\n",
      "\n",
      "hermione:\n",
      "verily!\n",
      "you put me off with limber vows; but i,\n",
      "though you would seek to unsphere the\n",
      "stars with oaths,\n",
      "should yet say 'sir, no going.' verily,\n",
      "you shall not go: a lady's 'verily' 's\n",
      "as potent as a lord's. will you\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "input:  menenius\n",
      "output:  menenius:\n",
      "why, what of that?\n",
      "\n",
      "first citizen:\n",
      "the former agents, if they did complain,\n",
      "what could the belly answer?\n",
      "\n",
      "menenius:\n",
      "i will tell you\n",
      "if you'll bestow a small--of what you have little--\n",
      "patience awhile, you'll hear the belly's answer.\n",
      "\n",
      "first citizen:\n",
      "ye're long about it.\n",
      "\n",
      "menenius:\n",
      "note me this, good friend;\n",
      "your most grave belly\n",
      "\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_ot_for_prompt(input_):\n",
    "    output = generate_text(model, config, input_, bpe, max_new_tokens=100, temperature=0.7, top_k=50)\n",
    "\n",
    "    output = clean_repetition(output)\n",
    "\n",
    "    print(\"input: \", input_)\n",
    "    print(\"output: \", output+\"\\n\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "prompt_lis = [\n",
    "   \"first citizen\",\n",
    "    \"hermione:\",\n",
    "    \"menenius\"\n",
    "]\n",
    "\n",
    "for prompt in prompt_lis:\n",
    "    print_ot_for_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b7f8b-c437-47c8-a1fe-598b5640a4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c455c1-c1bb-4c63-bb71-3fa3f528c14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add68159-b4d2-4545-97c2-4fdb05716f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7134321,
     "sourceId": 11391908,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
