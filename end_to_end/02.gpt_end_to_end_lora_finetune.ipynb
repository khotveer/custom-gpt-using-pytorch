{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af68762d-d942-4295-ab1f-637122f17529",
   "metadata": {},
   "source": [
    "### set seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1accbc-940e-43b3-b3ee-500c808c916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27f670-3a5c-4d99-927a-24b4b03002ca",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dd628f-ce53-49bc-bc63-7b7651a251ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import requests\n",
    "\n",
    "def bytes_to_unicode():\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:] \n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, cs))\n",
    "    return d\n",
    "\n",
    "def get_pairs(word):\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def get_file(local_file, remote_file):\n",
    "    if not os.path.isfile(local_file):\n",
    "        print(f\"downloading {remote_file} to {local_file}\")\n",
    "        response = requests.get(remote_file)\n",
    "        open(local_file, \"wb\").write(response.content)\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "\n",
    "    def __init__(self, encoder, bpe_merges):\n",
    "        # byte encoder/decoder\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        \n",
    "        # bpe token encoder/decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        # bpe merge list that defines the bpe \"tree\", of tuples (a,b) that are to merge to token ab\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self.cache = {}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        # token is a string of one individual 'word', after byte encoding, e.g. 'Ġthere'\n",
    "\n",
    "        # memoization, for efficiency\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token) # individual characters that make up the token, in a tuple\n",
    "        pairs = get_pairs(word) # get all bigrams\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # find the next lowest rank bigram that can be merged\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break # no more bigrams are eligible to be merged\n",
    "            first, second = bigram\n",
    "\n",
    "            # we will now replace all occurences of (first, second) in the list of current\n",
    "            # words into one merged token first_second, in the output list new_words\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "\n",
    "                # find the next occurence of first in the sequence of current words\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                # if this occurence is also followed by second, then merge them into one\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "\n",
    "            # all occurences of (first, second) have been merged to first_second\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "\n",
    "        # concat all words into a string, and use ' ' as the separator. Note that\n",
    "        # by now all characters have been byte encoded, guaranteeing that ' ' is\n",
    "        # not used in the actual data and is a 'special' delimiter character\n",
    "        word = ' '.join(word)\n",
    "\n",
    "        # cache the result and return\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_idx = []\n",
    "        # pre-tokenize the input text into string tokens (words, roughly speaking)\n",
    "        tokens = re.findall(self.pat, text)\n",
    "        # process each token into BPE integers\n",
    "        for token in tokens:\n",
    "            # encode the token as a bytes (b'') object\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            # translate all bytes to their unicode string representation and flatten\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "            # perform all the applicable bpe merges according to self.bpe_ranks\n",
    "            token_merged = self.bpe(token_translated).split(' ')\n",
    "            # translate all bpe tokens to integers\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "            # extend our running list of all output integers\n",
    "            bpe_idx.extend(token_ix)\n",
    "        return bpe_idx\n",
    "\n",
    "    def encode_and_show_work(self, text):\n",
    "        bpe_idx = []\n",
    "        parts = []\n",
    "        tokens = re.findall(self.pat, text)\n",
    "        for token in tokens:\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "            token_merged = self.bpe(token_translated).split(' ')\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "            bpe_idx.extend(token_ix)\n",
    "            parts.append({\n",
    "                'token': token,\n",
    "                'token_bytes': token_bytes,\n",
    "                'token_translated': token_translated,\n",
    "                'token_merged': token_merged,\n",
    "                'token_ix': token_ix,\n",
    "            })\n",
    "        out = {\n",
    "            'bpe_idx': bpe_idx, # the actual output sequence\n",
    "            'tokens': tokens, # result of pre-tokenization\n",
    "            'parts': parts, # intermediates for each token part\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    def decode(self, bpe_idx):\n",
    "        # inverse map the integers to get the tokens\n",
    "        tokens_merged = [self.decoder[token] for token in bpe_idx]\n",
    "        # inverse the byte encoder, e.g. recovering 'Ġ' -> ' ', and get the bytes\n",
    "        tokens_flat = ''.join(tokens_merged)\n",
    "        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])\n",
    "        # recover the full utf-8 string\n",
    "        text = tokens_bytes.decode('utf-8', errors='replace')\n",
    "        return text\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.encoder, self.decoder, self.vocab_size = self.get_encoder()\n",
    "        # self.decoder = enc_obj\n",
    "    \n",
    "    def get_file(self, local_file, remote_file):\n",
    "        if not os.path.isfile(local_file):\n",
    "            print(f\"downloading {remote_file} to {local_file}\")\n",
    "            response = requests.get(remote_file)\n",
    "            open(local_file, \"wb\").write(response.content)\n",
    "\n",
    "    def get_the_encoder(self):\n",
    "\n",
    "        directory = \"./temp/\"\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        # downloading the pairs, which is used in GPT-2 model\n",
    "        encoder_local_file = os.path.join(directory, 'encoder.json')\n",
    "        encoder_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'\n",
    "        self.get_file(encoder_local_file, encoder_remote_file)\n",
    "        \n",
    "        with open(\"./temp/encoder.json\", 'r') as f:\n",
    "            encoder = json.load(f)\n",
    "        \n",
    "        assert (len(encoder) == 50257), \"Encoder length donwloaded is not matching 50257\"\n",
    "        \n",
    "        # donwloading the vocab.bpe rules\n",
    "        vocab_local_file = os.path.join(directory, 'vocab.bpe')\n",
    "        vocab_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'\n",
    "        self.get_file(vocab_local_file, vocab_remote_file)\n",
    "        with open(vocab_local_file, 'r', encoding=\"utf-8\") as f:\n",
    "            bpe_data = f.read()\n",
    "        \n",
    "        bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "        \n",
    "        assert (len(bpe_merges) == 50000), \"BPE length donwloaded is not matching 50000\"\n",
    "        \n",
    "        encoder = Encoder(encoder, bpe_merges)\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def get_encoder(self):\n",
    "        enc_dec_obj = self.get_the_encoder()\n",
    "        encoder = enc_dec_obj.encode\n",
    "        decoder = enc_dec_obj.decode\n",
    "        return encoder, decoder, len(enc_dec_obj.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc35cd-ac5b-44a7-9595-5f15e40f5a5d",
   "metadata": {},
   "source": [
    "### Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86106e8-a29e-4554-859e-ecd71c3fc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc71d60-283d-4b63-8950-c290947d5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d4d0a3-3aae-4f08-9c21-50c5022d0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['n_embd'] % config['n_head'] == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
    "        \n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config['attn_pdrop'])\n",
    "        self.resid_dropout = nn.Dropout(config['resid_pdrop'])\n",
    "        \n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
    "                                     .view(1, 1, config['block_size'], config['block_size']))\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_embd = config['n_embd']\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a2a18f-1c16-4c69-8193-51fd27858c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
    "            c_proj  = nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config['resid_pdrop']),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb65f06-25f1-4ac0-9fcb-d71d2f7d2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = config['block_size']\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
    "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
    "            drop = nn.Dropout(config['embd_pdrop']),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
    "            ln_f = nn.LayerNorm(config['n_embd']),\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config['n_layer']))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        \n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        \n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        \n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config['weight_decay']},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config['learning_rate'], betas=train_config['betas'])\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "\n",
    "        b, t = idx.size()\n",
    "\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "\n",
    "        \n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "      \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "    \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad80e21a-e04a-41ef-8505-79d9cee07872",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, config, start_text, encoder, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "    seed = random.randint(0, 10_000)\n",
    "    set_seed(seed)\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode the input text\n",
    "    encoded = encoder.encoder(start_text)\n",
    "    x = torch.tensor(encoded, dtype=torch.long)[None, :].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x if x.size(1) <= config['block_size'] else x[:, -config['block_size']:]\n",
    "\n",
    "        logits, _ = model(x_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, top_indices = torch.topk(logits, top_k)\n",
    "            logits = torch.full_like(logits, float('-inf'))\n",
    "            logits.scatter_(1, top_indices, top_logits)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "    # Decode tokens to text\n",
    "    out = x[0].tolist()\n",
    "    return encoder.decoder(out)\n",
    "\n",
    "def clean_repetition(text):\n",
    "    # Clean up repeated \"Nobel Prize in <field>\"\n",
    "    text = re.sub(r'(the nobel prize in \\w+)( \\1)+', r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Clean other repetitive structures, e.g., \"the the\", \"was was\"\n",
    "    text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n",
    "\n",
    "    # Remove excessive repetition of \"in <year>\" in close succession\n",
    "    text = re.sub(r'(\\d{4},\\s*)\\1+', r'\\1', text)\n",
    "\n",
    "    # Clean up the format if there are multiple \"Nobel Prize\" mentions in different sections\n",
    "    text = re.sub(r'(the nobel prize in \\w+ ){2,}', r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958ae4b9-6f5f-4d16-82af-1ea5ccc680b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    " 'n_layer': 8,\n",
    " 'n_head': 16,\n",
    " 'n_embd': 512,\n",
    " 'vocab_size': 50257,\n",
    " 'block_size': 128,\n",
    " 'embd_pdrop': 0.1,\n",
    " 'resid_pdrop': 0.1,\n",
    " 'attn_pdrop': 0.1,\n",
    " 'device': 'cpu',\n",
    " 'num_workers': 3,\n",
    " 'max_iters': None,\n",
    " 'batch_size': 64,\n",
    " 'learning_rate': 0.0003,\n",
    " 'betas': (0.9, 0.95),\n",
    " 'weight_decay': 0.1,\n",
    " 'grad_norm_clip': 1.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657d8d40-dfa7-4a8a-a63a-83373755b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 51.02M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b91d5521-7d46-485e-bc5a-0efc3ca1a043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(torch.load('../saved_models//model_shakespeare_new_v5_latest.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11a6be3-9f1f-46f8-9edd-6aa7ebfb5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b1f28e-c094-4543-9767-91de8c3bc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('../saved_models/encoder_shakespeare_v5.pkl', 'rb') as f:\n",
    "    bpe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78658f0-bf72-4f2b-a421-00c593eb454b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc88b6d-140c-4f89-a1a0-883b2b323b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ec58b8-c4a8-44ee-a74c-e981ee5a6a27",
   "metadata": {},
   "source": [
    "### Basic Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6f429a6-73ca-4af6-9d2e-104f9617cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  first citizen\n",
      "output:  first citizen:\n",
      "no, no; by god's good grace his son shall reign.\n",
      "\n",
      "third citizen:\n",
      "woe to the land that's govern'd by a child!\n",
      "\n",
      "second citizen:\n",
      "in him there is a hope of government,\n",
      "that in his nonage council under him,\n",
      "and in his full and ripen'd years himself,\n",
      "no doubt, shall then and till then govern well.\n",
      "\n",
      "first citizen:\n",
      "so stood the state when henry the sixth\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "input:  hermione:\n",
      "output:  hermione:\n",
      "'tis none so hot. but, good sir,\n",
      "when you are cloudy.\n",
      "\n",
      "sebastian:\n",
      "foul weather?\n",
      "\n",
      "antonio:\n",
      "very foul.\n",
      "\n",
      "gonzalo:\n",
      "had i plantation of this isle, my lord,--\n",
      "\n",
      "antonio:\n",
      "he'ld sow't with nettle-seed.\n",
      "\n",
      "sebastian:\n",
      "or docks, or mallows.\n",
      "\n",
      "gonzalo:\n",
      "and were the\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "input:  menenius\n",
      "output:  menenius:\n",
      "and 'twas time for him too, i'll warrant him that:\n",
      "an he had stayed by him, i would not have been so\n",
      "fidiused for all the chests in corioli, and the gold\n",
      "that's in them. is the senate possessed of this?\n",
      "\n",
      "volumnia:\n",
      "good ladies, let's go. yes, yes, yes; the senate\n",
      "has letters from the general, wherein he gives my\n",
      "son the whole name of the\n",
      "\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_ot_for_prompt(input_):\n",
    "    output = generate_text(model, config, input_, bpe, max_new_tokens=100, temperature=0.7, top_k=50)\n",
    "\n",
    "    output = clean_repetition(output)\n",
    "        # Now cut the output at the first full stop.\n",
    "    # if '\\n' in output:\n",
    "    #     output = output.split('\\n')[0].strip()\n",
    "\n",
    "    print(\"input: \", input_)\n",
    "    print(\"output: \", output+\"\\n\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "prompt_lis = [\n",
    "   \"first citizen\",\n",
    "    \"hermione:\",\n",
    "    \"menenius\"\n",
    "]\n",
    "\n",
    "for prompt in prompt_lis:\n",
    "    print_ot_for_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dea762-5623-4a2c-9c23-d45c229c2a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add68159-b4d2-4545-97c2-4fdb05716f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7134321,
     "sourceId": 11391908,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
