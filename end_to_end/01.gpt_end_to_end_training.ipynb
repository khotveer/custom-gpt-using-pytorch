{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2269a-87b3-4c4c-83c1-fa6a07ed94a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af68762d-d942-4295-ab1f-637122f17529",
   "metadata": {},
   "source": [
    "### set seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1accbc-940e-43b3-b3ee-500c808c916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd628f-ce53-49bc-bc63-7b7651a251ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e413e62-0f7b-4f84-bcaf-0deb4592cc42",
   "metadata": {},
   "source": [
    "#### BPE encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5f1c1e-28ef-42c1-84b7-1e080a30ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import requests\n",
    "\n",
    "def bytes_to_unicode():\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:] \n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, cs))\n",
    "    return d\n",
    "\n",
    "def get_pairs(word):\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def get_file(local_file, remote_file):\n",
    "    if not os.path.isfile(local_file):\n",
    "        print(f\"downloading {remote_file} to {local_file}\")\n",
    "        response = requests.get(remote_file)\n",
    "        open(local_file, \"wb\").write(response.content)\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "\n",
    "    def __init__(self, encoder, bpe_merges):\n",
    "        # byte encoder/decoder\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        \n",
    "        # bpe token encoder/decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        # bpe merge list that defines the bpe \"tree\", of tuples (a,b) that are to merge to token ab\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self.cache = {}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        # token is a string of one individual 'word', after byte encoding, e.g. 'Ġthere'\n",
    "\n",
    "        # memoization, for efficiency\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token) # individual characters that make up the token, in a tuple\n",
    "        pairs = get_pairs(word) # get all bigrams\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # find the next lowest rank bigram that can be merged\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break # no more bigrams are eligible to be merged\n",
    "            first, second = bigram\n",
    "\n",
    "            # we will now replace all occurences of (first, second) in the list of current\n",
    "            # words into one merged token first_second, in the output list new_words\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "\n",
    "                # find the next occurence of first in the sequence of current words\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                # if this occurence is also followed by second, then merge them into one\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "\n",
    "            # all occurences of (first, second) have been merged to first_second\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "\n",
    "        # concat all words into a string, and use ' ' as the separator. Note that\n",
    "        # by now all characters have been byte encoded, guaranteeing that ' ' is\n",
    "        # not used in the actual data and is a 'special' delimiter character\n",
    "        word = ' '.join(word)\n",
    "\n",
    "        # cache the result and return\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_idx = []\n",
    "        # pre-tokenize the input text into string tokens (words, roughly speaking)\n",
    "        tokens = re.findall(self.pat, text)\n",
    "        # process each token into BPE integers\n",
    "        for token in tokens:\n",
    "            # encode the token as a bytes (b'') object\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            # translate all bytes to their unicode string representation and flatten\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "            # perform all the applicable bpe merges according to self.bpe_ranks\n",
    "            token_merged = self.bpe(token_translated).split(' ')\n",
    "            # translate all bpe tokens to integers\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "            # extend our running list of all output integers\n",
    "            bpe_idx.extend(token_ix)\n",
    "        return bpe_idx\n",
    "\n",
    "    def encode_and_show_work(self, text):\n",
    "        bpe_idx = []\n",
    "        parts = []\n",
    "        tokens = re.findall(self.pat, text)\n",
    "        for token in tokens:\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "            token_merged = self.bpe(token_translated).split(' ')\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "            bpe_idx.extend(token_ix)\n",
    "            parts.append({\n",
    "                'token': token,\n",
    "                'token_bytes': token_bytes,\n",
    "                'token_translated': token_translated,\n",
    "                'token_merged': token_merged,\n",
    "                'token_ix': token_ix,\n",
    "            })\n",
    "        out = {\n",
    "            'bpe_idx': bpe_idx, # the actual output sequence\n",
    "            'tokens': tokens, # result of pre-tokenization\n",
    "            'parts': parts, # intermediates for each token part\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    def decode(self, bpe_idx):\n",
    "        # inverse map the integers to get the tokens\n",
    "        tokens_merged = [self.decoder[token] for token in bpe_idx]\n",
    "        # inverse the byte encoder, e.g. recovering 'Ġ' -> ' ', and get the bytes\n",
    "        tokens_flat = ''.join(tokens_merged)\n",
    "        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])\n",
    "        # recover the full utf-8 string\n",
    "        text = tokens_bytes.decode('utf-8', errors='replace')\n",
    "        return text\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.encoder, self.decoder, self.vocab_size = self.get_encoder()\n",
    "        # self.decoder = enc_obj\n",
    "    \n",
    "    def get_file(self, local_file, remote_file):\n",
    "        if not os.path.isfile(local_file):\n",
    "            print(f\"downloading {remote_file} to {local_file}\")\n",
    "            response = requests.get(remote_file)\n",
    "            open(local_file, \"wb\").write(response.content)\n",
    "\n",
    "    def get_the_encoder(self):\n",
    "\n",
    "        directory = \"./temp/\"\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        # downloading the pairs, which is used in GPT-2 model\n",
    "        encoder_local_file = os.path.join(directory, 'encoder.json')\n",
    "        encoder_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'\n",
    "        self.get_file(encoder_local_file, encoder_remote_file)\n",
    "        \n",
    "        with open(\"./temp/encoder.json\", 'r') as f:\n",
    "            encoder = json.load(f)\n",
    "        \n",
    "        assert (len(encoder) == 50257), \"Encoder length donwloaded is not matching 50257\"\n",
    "        \n",
    "        # donwloading the vocab.bpe rules\n",
    "        vocab_local_file = os.path.join(directory, 'vocab.bpe')\n",
    "        vocab_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'\n",
    "        self.get_file(vocab_local_file, vocab_remote_file)\n",
    "        with open(vocab_local_file, 'r', encoding=\"utf-8\") as f:\n",
    "            bpe_data = f.read()\n",
    "        \n",
    "        bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "        \n",
    "        assert (len(bpe_merges) == 50000), \"BPE length donwloaded is not matching 50000\"\n",
    "        \n",
    "        encoder = Encoder(encoder, bpe_merges)\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def get_encoder(self):\n",
    "        enc_dec_obj = self.get_the_encoder()\n",
    "        encoder = enc_dec_obj.encode\n",
    "        decoder = enc_dec_obj.decode\n",
    "        return encoder, decoder, len(enc_dec_obj.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1caed-39f2-46ba-afad-5b27755e9e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cefc35cd-ac5b-44a7-9595-5f15e40f5a5d",
   "metadata": {},
   "source": [
    "### Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86106e8-a29e-4554-859e-ecd71c3fc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc71d60-283d-4b63-8950-c290947d5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d4d0a3-3aae-4f08-9c21-50c5022d0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config['n_embd'] % config['n_head'] == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
    "        \n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config['attn_pdrop'])\n",
    "        self.resid_dropout = nn.Dropout(config['resid_pdrop'])\n",
    "        \n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
    "                                     .view(1, 1, config['block_size'], config['block_size']))\n",
    "        self.n_head = config['n_head']\n",
    "        self.n_embd = config['n_embd']\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a2a18f-1c16-4c69-8193-51fd27858c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
    "            c_proj  = nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config['resid_pdrop']),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb65f06-25f1-4ac0-9fcb-d71d2f7d2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = config['block_size']\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
    "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
    "            drop = nn.Dropout(config['embd_pdrop']),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
    "            ln_f = nn.LayerNorm(config['n_embd']),\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config['n_layer']))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        \n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        \n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        \n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config['weight_decay']},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config['learning_rate'], betas=train_config['betas'])\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "\n",
    "        b, t = idx.size()\n",
    "\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "\n",
    "        \n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "      \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "    \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04029bc0-d4c5-4b26-9f29-feaa20a389e2",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3206d81e-fb56-40a0-bd2f-29db1c55b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config, model, train_dataset, test_dataset):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "\n",
    "        self.device = config['device']\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(\"running on device\", self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        val_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                \n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                _, loss = model(x, y)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "\n",
    "        avg_val_loss = total_loss / count\n",
    "        # print(f\"[Validation] Iter {self.iter_num}: val loss = {avg_val_loss:.4f}\")\n",
    "        model.train()\n",
    "        return avg_val_loss\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "       \n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "       \n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=batch_size)\n",
    "     \n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        self.iter_num = 0\n",
    "\n",
    "        self.iter_time = time.time()\n",
    "        \n",
    "        data_iter = iter(train_loader)\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "                \n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "               \n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            \n",
    "            x, y = batch\n",
    "\n",
    "            logits, self.loss = model(x, y)\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            self.loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_norm_clip'])\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "            \n",
    "            self.iter_num += 1\n",
    "            \n",
    "            tnow = time.time()\n",
    "            self.iter_dt = tnow - self.iter_time\n",
    "            self.iter_time = tnow\n",
    "\n",
    "            if self.iter_num % 100 == 0:\n",
    "                avg_val_loss = self.validate(self.test_dataset)\n",
    "                print(self.iter_num, f\": loss: {self.loss.item()} val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            # termination conditions\n",
    "            if config['max_iters'] is not None and self.iter_num >= config['max_iters']:\n",
    "                break\n",
    "              # Optional: Run validation every 500 steps\n",
    "            if self.iter_num % 100 == 0:\n",
    "                self.validate(self.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d259e201-b0ab-4779-bcdb-3dac47a637ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x = self.data[idx : idx + self.block_size]\n",
    "        # y = self.data[idx + 1 : idx + 1 + self.block_size]\n",
    "        x = torch.tensor(self.data[idx : idx + self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx + 1 : idx + 1 + self.block_size], dtype=torch.long)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe4fa0-f68d-4fa2-bddf-30a7ab037087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee4b517-0ecc-415f-a6ab-568d20156c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_layer\": 8,\n",
    "    \"n_head\": 16,\n",
    "    \"n_embd\": 512,\n",
    "\n",
    "    \n",
    "    \"vocab_size\" : None,\n",
    "    \"block_size\" : None,\n",
    "\n",
    "    \n",
    "    \"embd_pdrop\" : 0.1,\n",
    "    \"resid_pdrop\" : 0.1,\n",
    "    \"attn_pdrop\" : 0.1,\n",
    "\n",
    "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"num_workers\" : 3,\n",
    "        \n",
    "    # optimizer parameters\n",
    "    \"max_iters\" : None,\n",
    "    \"batch_size\" : 64,\n",
    "    \"learning_rate\" : 3e-4,\n",
    "    \"betas\" : (0.9, 0.95),\n",
    "    \"weight_decay\" : 0.1, # only applied on matmul weights\n",
    "    \"grad_norm_clip\" : 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "747e9b81-8a9c-459f-a48c-6f776f20c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a608b06a-3150-4da9-b61e-179adad734f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e07e718-818b-4bca-af6c-ac5f883e1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/shakespeare.txt\", \n",
    "          \"r\", encoding = 'utf-8') as f:\n",
    "    text  = f.read()\n",
    "\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31fd12d7-b369-4bd8-b331-199096d6a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json to ./temp/encoder.json\n",
      "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe to ./temp/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "bpe = BPE()\n",
    "encoder = bpe.encoder\n",
    "decoder = bpe.decoder\n",
    "vocab_size = bpe.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e49de2-d594-4980-8783-8e50db2bc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = bpe.get_the_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82070798-6efc-45ba-88de-be74eaeed98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = encoder(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35595644-246b-4a09-9ae7-c082282dbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "batch_size = config['batch_size']\n",
    "dataset = ShakespeareDataset(encoded_dataset, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbb4e01-2659-4261-a17c-63918da9a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "668935c8-dc00-47f3-8fe5-048888931c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split sizes (e.g., 90% train, 10% test)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9fe84f8-3e68-426b-b1c9-a14fae51686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8adec-c456-4e9c-b1ca-8333bef36f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a203298-7fb1-4b47-8528-599a9d2945e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['vocab_size'] = vocab_size\n",
    "config['block_size'] = block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "657d8d40-dfa7-4a8a-a63a-83373755b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 51.02M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdfeb9-da00-4683-b3d6-a0b791bbb0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "970a9d42-57f0-4041-9d30-5a5835b36837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:21:01.054499Z",
     "iopub.status.busy": "2025-04-26T16:21:01.053836Z",
     "iopub.status.idle": "2025-04-26T16:21:01.341584Z",
     "shell.execute_reply": "2025-04-26T16:21:01.340916Z",
     "shell.execute_reply.started": "2025-04-26T16:21:01.054474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config, model, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac7f5404-06ee-4cd2-888d-f280c0ca80c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T16:21:15.699729Z",
     "iopub.status.busy": "2025-04-26T16:21:15.699459Z",
     "iopub.status.idle": "2025-04-26T23:18:28.133540Z",
     "shell.execute_reply": "2025-04-26T23:18:28.132340Z",
     "shell.execute_reply.started": "2025-04-26T16:21:15.699709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 : loss: 5.036053657531738 val loss: 4.9268\n",
      "200 : loss: 4.283403396606445 val loss: 4.3206\n",
      "300 : loss: 3.9661524295806885 val loss: 3.9668\n",
      "400 : loss: 3.8937549591064453 val loss: 3.7188\n",
      "500 : loss: 3.461179494857788 val loss: 3.4843\n",
      "600 : loss: 3.4044785499572754 val loss: 3.2734\n",
      "700 : loss: 3.0049102306365967 val loss: 3.0334\n",
      "800 : loss: 2.960927963256836 val loss: 2.7833\n",
      "900 : loss: 2.708482503890991 val loss: 2.5152\n",
      "1000 : loss: 2.444145441055298 val loss: 2.2310\n",
      "1100 : loss: 2.1505088806152344 val loss: 1.9292\n",
      "1200 : loss: 1.9313178062438965 val loss: 1.6313\n",
      "1300 : loss: 1.6364212036132812 val loss: 1.3412\n",
      "1400 : loss: 1.508245587348938 val loss: 1.0886\n",
      "1500 : loss: 1.2650781869888306 val loss: 0.8704\n",
      "1600 : loss: 1.0747039318084717 val loss: 0.6844\n",
      "1700 : loss: 0.9347623586654663 val loss: 0.5423\n",
      "1800 : loss: 0.7267189621925354 val loss: 0.4402\n",
      "1900 : loss: 0.6485491394996643 val loss: 0.3674\n",
      "2000 : loss: 0.6270428895950317 val loss: 0.3145\n",
      "2100 : loss: 0.5415418148040771 val loss: 0.2799\n",
      "2200 : loss: 0.5057888627052307 val loss: 0.2541\n",
      "2300 : loss: 0.43348217010498047 val loss: 0.2339\n",
      "2400 : loss: 0.4529948830604553 val loss: 0.2200\n",
      "2500 : loss: 0.4100381135940552 val loss: 0.2101\n",
      "2600 : loss: 0.3789180815219879 val loss: 0.1981\n",
      "2700 : loss: 0.3657515347003937 val loss: 0.1932\n",
      "2800 : loss: 0.33658385276794434 val loss: 0.1863\n",
      "2900 : loss: 0.33644065260887146 val loss: 0.1818\n",
      "3000 : loss: 0.3279247283935547 val loss: 0.1760\n",
      "3100 : loss: 0.3183009624481201 val loss: 0.1743\n",
      "3200 : loss: 0.3063071072101593 val loss: 0.1702\n",
      "3300 : loss: 0.28456494212150574 val loss: 0.1653\n",
      "3400 : loss: 0.27909380197525024 val loss: 0.1617\n",
      "3500 : loss: 0.28388512134552 val loss: 0.1600\n",
      "3600 : loss: 0.26362988352775574 val loss: 0.1568\n",
      "3700 : loss: 0.2597239911556244 val loss: 0.1532\n",
      "3800 : loss: 0.25283166766166687 val loss: 0.1519\n",
      "3900 : loss: 0.24901606142520905 val loss: 0.1503\n",
      "4000 : loss: 0.24917611479759216 val loss: 0.1476\n",
      "4100 : loss: 0.23215129971504211 val loss: 0.1454\n",
      "4200 : loss: 0.24091669917106628 val loss: 0.1432\n",
      "4300 : loss: 0.24469821155071259 val loss: 0.1419\n",
      "4400 : loss: 0.23816703259944916 val loss: 0.1402\n",
      "4500 : loss: 0.2290792614221573 val loss: 0.1388\n",
      "4600 : loss: 0.22734512388706207 val loss: 0.1375\n",
      "4700 : loss: 0.2110547423362732 val loss: 0.1373\n",
      "4800 : loss: 0.19730396568775177 val loss: 0.1354\n",
      "4900 : loss: 0.21275772154331207 val loss: 0.1339\n",
      "5000 : loss: 0.2081051617860794 val loss: 0.1339\n",
      "5100 : loss: 0.2056298851966858 val loss: 0.1324\n",
      "5200 : loss: 0.1929665058851242 val loss: 0.1320\n",
      "5300 : loss: 0.20132765173912048 val loss: 0.1304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3609282671.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/2319137733.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m               \u001b[0;31m# Optional: Run validation every 500 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/2319137733.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, val_loader)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2796c-dab4-49d1-86c5-7dcee92538a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58e78a60-2f11-493b-ba50-d13c82f0bce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T23:18:49.265070Z",
     "iopub.status.busy": "2025-04-26T23:18:49.264791Z",
     "iopub.status.idle": "2025-04-26T23:18:49.824107Z",
     "shell.execute_reply": "2025-04-26T23:18:49.823485Z",
     "shell.execute_reply.started": "2025-04-26T23:18:49.265049Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../saved_models/model_shakespeare_new_v5_latest.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aa6f143-e027-4578-8360-83ca03c3b214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T23:18:57.803471Z",
     "iopub.status.busy": "2025-04-26T23:18:57.803239Z",
     "iopub.status.idle": "2025-04-26T23:18:57.861382Z",
     "shell.execute_reply": "2025-04-26T23:18:57.860623Z",
     "shell.execute_reply.started": "2025-04-26T23:18:57.803455Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Let's say your encoder/tokenizer is in a variable called `encoder`\n",
    "with open(\"../saved_models/encoder_shakespeare_v5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bpe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c10312-1430-4bdb-a1bc-4c09d2ab3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b34f3-0e4d-4d05-94f0-f5b0e4ad363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7d011-3e7e-4b70-9239-cea2c174f56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef55b16d-15f4-46ee-94f0-75d2bb0475a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d562a-5363-471a-a6f6-e88a1aa20b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7134321,
     "sourceId": 11391908,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7231602,
     "sourceId": 11529731,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
